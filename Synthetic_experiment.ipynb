{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct ConnectHub, a synthetic dataset where for each knowledge graph $G = (V(G),E(G),R(G))$ in the dataset, we can partition the relation that appears into two classes $R(G) = R_H(G) \\bigcup R_N(G) \\bigcup \\{r_0\\}$. Then, each of these graphs consists of the following components:\n",
    "\n",
    "- A \"hub\" graph $H$ where we have the center node $u$, and for all relation $r \\in R_H(G)$, $\\exists v \\in V(G)$ such that $r(v,u)$ holds.\n",
    "- Multiple \"pair\" graphs with \"shared hub relations\" $S_H$ where for each pair of $r_1, r_2 \\in R_H(G)$, we have a center node $x$, and $\\exists y,z \\in V(G)$ such that $r_1(y,x) \\land r_2(z,x)$ hold.\n",
    "- Multiple \"pair\" graphs with ``different hub relations'' $S_N$ where for each pair of $r_1, r_2 \\in R_N(G)$, we have a center node $x'$, and $\\exists y',z' \\in V(G)$ such that $r_1(y',x') \\land r_2(z',x')$ hold.\n",
    "\n",
    "Finally, the graph $G$ is a disjoint union of all these components. The objective of the task is to predict the link $r_0(u,x)$ from the center node  $u$ in the hub graph to the center node $x$ of the pair graph in $S_H$ as true, and the link $r_0(u,x')$ from the center node $u$ in the hub graph to the center node $x'$ of pair graph in $S_N$ false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F   \n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torch_geometric.loader import DataLoader\n",
    "from motif.tasks import build_relation_graph, build_relation_hypergraph_synth\n",
    "from motif.models import Ultra, MOTIF\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a subgraph dataset\n",
    "\n",
    "def concat_two_graph(index_1, type_1, index_2, type_2):\n",
    "    node_1 = torch.max(index_1.view(-1)) + 1\n",
    "    index = torch.cat([index_1, index_2 + node_1], dim=-1)\n",
    "    edge_type = torch.cat([type_1, type_2])\n",
    "    return index, edge_type, node_1\n",
    "\n",
    "def generate_hub_graph(relation_list):\n",
    "    num_nodes = len(relation_list) + 1\n",
    "    num_edges = len(relation_list)\n",
    "    index = torch.tensor([[i, 0] for i in range(1, num_nodes)], dtype = torch.int64).t().contiguous()\n",
    "    edge_type = torch.tensor(relation_list, dtype = torch.int64)\n",
    "    return index, edge_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_permutations(lst, k):\n",
    "    # Generate permutations using itertools\n",
    "\n",
    "    all_permutations = []\n",
    "    for i in range(2,k+2):\n",
    "        all_permutations += list(itertools.combinations(lst, i))\n",
    "\n",
    "\n",
    "    return all_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_instances(num_instances, k):\n",
    "    # Here, we will generate num_instances times num_instances data\n",
    "    graph_list = []\n",
    "    for R_H_count in tqdm.tqdm(range(k+1, k+num_instances+1)):\n",
    "        R_N_count = R_H_count\n",
    "        R_H = [i for i in range(1, R_H_count+1)]\n",
    "        R_N = [i + R_H_count for i in range(1, R_N_count+1)]\n",
    "        target_edge_index = []\n",
    "        not_target_edge_index = []\n",
    "        # generate hub graph\n",
    "        index, edge_type = generate_hub_graph(R_H)\n",
    "        # generate pair graph\n",
    "        R_H_permutations = generate_permutations(R_H, k-1)\n",
    "        for rel_list in R_H_permutations:\n",
    "            index_pair, edge_type_pair = generate_hub_graph(rel_list)\n",
    "            index, edge_type, node_1  = concat_two_graph(index, edge_type, index_pair, edge_type_pair)\n",
    "            target_edge_index.append([0, node_1])\n",
    "        R_N_permutations = generate_permutations(R_N, k-1)\n",
    "        for rel_list in R_N_permutations:\n",
    "            index_pair, edge_type_pair = generate_hub_graph(rel_list)\n",
    "            index, edge_type, node_1  = concat_two_graph(index, edge_type, index_pair, edge_type_pair)\n",
    "            not_target_edge_index.append([0, node_1])\n",
    "        \n",
    "        graph = Data(\n",
    "            edge_index=index,\n",
    "            edge_type=edge_type,\n",
    "            num_relations=torch.max(edge_type).item() + 1,\n",
    "            num_nodes=torch.max(index.view(-1)).item() + 1,\n",
    "            target_edge_index= torch.tensor(target_edge_index).T,\n",
    "            target_edge_type=torch.zeros(len(target_edge_index)),\n",
    "            not_target_edge_index= torch.tensor(not_target_edge_index).T,\n",
    "            not_target_edge_type=torch.zeros(len(not_target_edge_index)),\n",
    "            device = device,\n",
    "            relation_hypergraph=[],\n",
    "        )\n",
    "        graph = build_relation_graph(graph)\n",
    "        for current_arity in range(2, k+2):\n",
    "            graph = build_relation_hypergraph_synth(graph, max_arity=current_arity)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(data_instances, k, seed=0):\n",
    "    graph_list = create_data_instances(data_instances, k)\n",
    "    random.Random(seed).shuffle(graph_list)\n",
    "    split = int(len(graph_list) * 0.7) # Don't do shuffling to see if this can generalize well to unseen large graphs\n",
    "    train_list = graph_list[:split]\n",
    "    test_list = graph_list[split:] \n",
    "    train_dataloader = DataLoader(train_list)\n",
    "    test_dataloader = DataLoader(test_list)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_sampling(graph_data):\n",
    "    target_edge_index = graph_data.target_edge_index\n",
    "    target_edge_type = graph_data.target_edge_type\n",
    "    not_target_edge_index = graph_data.not_target_edge_index\n",
    "    not_target_edge_type = graph_data.not_target_edge_type\n",
    "\n",
    "    h_index = torch.cat([target_edge_index[0], not_target_edge_index[0]]).unsqueeze(-1)\n",
    "    t_index = torch.cat([target_edge_index[1], not_target_edge_index[1]]).unsqueeze(-1)\n",
    "    r_index = torch.cat([target_edge_type, not_target_edge_type]).unsqueeze(-1)\n",
    "    target = torch.zeros(h_index.size(0))\n",
    "    target[: target_edge_index.size(-1)] = 1\n",
    "\n",
    "    return torch.stack([h_index.to(torch.int64), t_index.to(torch.int64), r_index.to(torch.int64)], dim=-1), target.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    num_epoch,\n",
    "    lr,\n",
    "):\n",
    "    if num_epoch == 0:\n",
    "        return\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    batch_id = 0\n",
    "    for epoch in range(0, num_epoch):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} begin\")\n",
    "        losses = []\n",
    "        for graph_data in train_dataloader:\n",
    "            batch, target = get_target_sampling(graph_data)\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "            graph_data = graph_data.to(device)\n",
    "            pred = model(graph_data, batch)\n",
    "            \n",
    "            loss = F.binary_cross_entropy_with_logits(pred, target).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            batch_id += 1\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        print(\"Epoch %d end\" % epoch)\n",
    "        print(\"average binary cross entropy: %g\" % avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    corrects = []\n",
    "    totals = []\n",
    "    for graph_data in test_dataloader:\n",
    "        batch, target = get_target_sampling(graph_data)\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        graph_data = graph_data.to(device)\n",
    "        pred = model(graph_data, batch)\n",
    "        output = (F.sigmoid(pred) > 0.5).float()\n",
    "        correct = (output == target).float().sum()\n",
    "        corrects.append(correct)\n",
    "        totals.append(target.size(0))\n",
    "    accuracy = 100 * sum(corrects) / sum(totals)\n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = generate_dataloader(\n",
    "    data_instances = 2,\n",
    "    k = 3,\n",
    ")\n",
    "# generate the hub with k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ultra_sum = Ultra(\n",
    "    rel_model_cfg=dict(\n",
    "        input_dim = 32,\n",
    "        hidden_dims = [32, 32],\n",
    "        message_func = \"distmult\",\n",
    "        aggregate_func = \"sum\",\n",
    "        short_cut = True,\n",
    "    ),\n",
    "    entity_model_cfg=dict(\n",
    "        input_dim = 32,\n",
    "        hidden_dims = [32, 32],\n",
    "        message_func = \"transe\",\n",
    "        aggregate_func = \"sum\",\n",
    "        layer_norm = True,\n",
    "    ),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTIF_sum = MOTIF(\n",
    "    rel_model_cfg=dict(\n",
    "        input_dim = 32,\n",
    "        hidden_dims = [32, 32],\n",
    "        aggregate_func = \"sum\",\n",
    "        max_considered_arity = 3, # to be expressive it has to be k+1\n",
    "        short_cut = True,\n",
    "        layer_norm = True, \n",
    "        use_triton = True,\n",
    "        synthetic = True,\n",
    "    ),\n",
    "    entity_model_cfg=dict(\n",
    "        input_dim = 32,\n",
    "        hidden_dims = [32, 32],\n",
    "        message_func = \"transe\",\n",
    "        aggregate_func = \"sum\",\n",
    "        layer_norm = True,\n",
    "        synthetic = True,\n",
    "    ),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(MOTIF_sum, train_dataloader, 500, 0.001)\n",
    "test(MOTIF_sum, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(end_k = 6):\n",
    "    result = []\n",
    "    for k in tqdm.tqdm(range(2, end_k+1)):\n",
    "        if k ==2 or k == 3:\n",
    "            data_instance = 10\n",
    "        elif k == 4:\n",
    "            data_instance = 6\n",
    "        else:\n",
    "            data_instance = 2\n",
    "        \n",
    "        train_dataloader, test_dataloader = generate_dataloader(\n",
    "            data_instances = data_instance,\n",
    "            k = k,\n",
    "        )\n",
    "        temp_result = []\n",
    "\n",
    "        for test_val in range(2, k+2):\n",
    "            print(\"k = {}, test = {}\".format(k, test_val))\n",
    "            MOTIF_sum = MOTIF(\n",
    "            rel_model_cfg=dict(\n",
    "                input_dim = 32,\n",
    "                hidden_dims = [32, 32],\n",
    "                aggregate_func = \"sum\",\n",
    "                max_considered_arity = test_val, # to be expressive it has to be k+1\n",
    "                short_cut = True,\n",
    "                layer_norm = True, \n",
    "                use_triton = True,\n",
    "                synthetic = True,\n",
    "            ),\n",
    "            entity_model_cfg=dict(\n",
    "                input_dim = 32,\n",
    "                hidden_dims = [32, 32],\n",
    "                message_func = \"transe\",\n",
    "                aggregate_func = \"sum\",\n",
    "                layer_norm = True,\n",
    "                synthetic = True,\n",
    "            ),\n",
    "            ).to(device)\n",
    "            train_and_validate(MOTIF_sum, train_dataloader, 1000, 0.001)\n",
    "            accuracy = test(MOTIF_sum, test_dataloader)\n",
    "            print(\"k = {}, test = {}, accuracy = {}\".format(k, test_val, accuracy))\n",
    "            temp_result.append(accuracy)\n",
    "        result.append(temp_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = experiments(7)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypergraph-envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
